# **Project Waypoint Product Requirements Document (PRD)**

## **Goal, Objective and Context**

**Overall Project Goal:** To preserve the "almost 25 years" of invaluable knowledge from The Magic Cafe forum by first creating a complete, locally-hosted raw HTML archive and a structured data version ("Waypoint Archive"). Subsequently, this archive will serve as the foundation for "Project Waypoint," an AI-powered research tool, making this content accessible and deeply searchable for the magic community.

**Immediate Objectives (for the "Waypoint Archive" phase):**

1. **Comprehensive Data Indexing:** To develop and execute a process that reliably identifies and catalogs all unique topics (threads) across all sub-forums of The Magic Cafe.  
2. **Complete Data Preservation:** To download and securely store the full raw HTML content of every page for every identified topic on the user's local Synology NAS, ensuring no data is left behind.  
3. **Initial Data Structuring:** To process the raw HTML archive locally into a structured, machine-readable format (e.g., JSON per thread), accurately parsing key metadata and content elements like quoted text and attributions.

**Context:** The Magic Cafe forum, a significant resource for the magic community for nearly 25 years, is at high risk of permanent data loss. This risk stems from its outdated technology, the owner's reported personal and financial circumstances, and its recent de-indexing by Google, which limits its visibility and utility. Project Waypoint is initiated with urgency to mitigate this risk. The initial "Waypoint Archive" phase will operate under a "no external budget" constraint, leveraging the user's existing hardware (laptop for indexing, Synology NAS for storage and processing) and development time. Subsequent phases to develop the AI tool will incur API costs, which the user is prepared to cover. The archival process must be conducted "politely" to minimize impact on the live forum server.

## **Functional Requirements (MVP \- "Waypoint Archive")**

These requirements outline the necessary functionalities of the scripts and processes involved in creating the "Waypoint Archive."

**1\. Indexing System (Topic ID & URL Collection):** 1.1. The system MUST be able to navigate through all pages of a given Magic Cafe sub-forum listing (e.g., `viewforum.php` pages). 1.2. The system MUST accurately identify and extract all unique topic (thread) IDs and their direct URLs (if readily available and distinct, with ID being primary) from the HTML of these sub-forum pages. 1.3. The system MUST implement a two-pass strategy for each sub-forum: 1.3.1. An initial pass to scan all pages of the sub-forum and collect unique topic IDs. 1.3.2. A subsequent, immediate re-scan of the first page of that sub-forum to identify and collect any topic IDs that were "bumped" to the front or newly created during the initial pass. 1.4. The system MUST store the collected unique topic IDs in a persistent, de-duplicated list (e.g., a text file or simple database) that can be consumed by the Archival System. 1.5. The system MUST be configurable to run on the user's laptop. 1.6. The system MUST provide clear logging of its progress (e.g., current sub-forum being processed, number of pages scanned, number of unique topic IDs found) and any errors encountered. 1.7. The system MUST be able to handle common HTML structures found on forum listing pages and identify relevant links. 1.8. The system MUST track its indexing rate (e.g., topics or pages of topics indexed per unit of time). 1.9. The system MUST provide an estimated time to completion (ETC) for the current sub-forum indexing task, dynamically updated based on its ongoing measured performance and the total known items (e.g., pages) for that sub-forum. 1.10. The system (or an associated utility) MUST be able to provide an initial ETC for indexing any new, unstarted sub-forum, based on the known number of topics/pages in that sub-forum and historical performance data from previous indexing runs.

**2\. Archival System (Raw HTML Download & Storage):** 2.1. The system MUST read the master list of unique topic URLs/IDs generated by the Indexing System. 2.2. For each topic URL/ID, the system MUST navigate all of its internal pages (e.g., page 1, 2, 3 of a thread). 2.3. The system MUST download the complete, unaltered raw HTML content for every page of every specified topic. 2.4. The system MUST save these raw HTML files to a user-specified location on the Synology NAS. 2.5. The saved HTML files MUST be organized in a logical directory structure (e.g., `ARCHIVE_ROOT/{sub_forum_name_or_id}/{topic_id}/page_{page_number}.html`). 2.6. The system MUST implement "polite scraping" features: 2.6.1. Configurable delay (e.g., 2-5 seconds, adjustable) between HTTP requests. 2.6.2. Ability to be scheduled or run during user-defined off-peak hours. 2.6.3. Send a custom User-Agent string with each request. 2.7. The system MUST implement state management to allow for resumable operation (i.e., if stopped, it can resume from the last successfully archived topic/page without re-downloading already completed content). 2.8. The system MUST provide detailed logging of its progress (e.g., current topic and page being downloaded, files saved) and any errors (e.g., HTTP errors, network issues). 2.9. The system MUST track its archival rate (e.g., pages or threads archived per unit of time, data volume downloaded per unit of time). 2.10. The system MUST provide an ETC for the current archival batch (e.g., for a specific sub-forum's list of threads), dynamically updated based on its ongoing measured performance and the total known pages to archive. 2.11. The system (or an associated utility) MUST be able to provide an initial ETC for archiving any new, unstarted batch of threads, based on the known number of pages in that batch and historical performance data from previous archival runs.

**3\. Structured Data Extraction System (Local Processing):** 3.1. The system MUST read the raw HTML files from the local "Waypoint Archive" stored on the Synology NAS. 3.2. For each archived HTML page, the system MUST parse it to extract individual posts. 3.3. For each post, the system MUST extract the following metadata: Post ID, page number within its thread, direct URL to the original post (if reconstructable), username of the poster, and the timestamp of the post. 3.4. The system MUST parse the main content of each post into structured `content_blocks`, accurately differentiating between: 3.4.1. `new_text` blocks (content authored by the current poster). 3.4.2. `quote` blocks. 3.5. For each `quote` block, the system MUST extract: 3.5.1. The username of the user being quoted (e.g., "MeetMagicMike"). 3.5.2. The timestamp or date attribution of the quoted post (e.g., "On Jun 11, 2017"). 3.5.3. The full text content of the quote itself. 3.6. The system MUST save the extracted structured data (metadata and `content_blocks` for all posts in a thread) into a machine-readable format (e.g., one JSON file per thread) in a user-specified location on the Synology NAS. 3.7. The system MUST be robust enough to handle the known HTML structure and variations present in The Magic Cafe forum posts, particularly regarding quote structures. 3.8. The system MUST provide logging of its progress (e.g., current HTML file being processed, number of posts extracted, JSON files created) and any parsing errors or anomalies. *(Acknowledged: The specific details within section 3, particularly field extraction and quote handling nuances, are preliminary and will be finalized after Phase 1 and 2 provide the complete raw HTML dataset for detailed analysis.)*

## **Non Functional Requirements (MVP \- "Waypoint Archive")**

These non-functional requirements (NFRs) apply to the scripts and processes used to create and manage the "Waypoint Archive."

**1\. Performance:** 1.1. **Scraping Efficiency:** While adhering to "polite scraping" delays (e.g., 2-5 seconds per request), the indexing and archival scripts should be optimized to maximize data throughput within the defined off-peak operational windows. 1.2. **Processing Efficiency:** The local structured data extraction script should process the archived HTML files efficiently on the user's Synology NAS, aiming to complete the structuring of the entire archive within a reasonable timeframe (to be benchmarked during testing). 1.3. **Resource Utilization:** Scripts running on the Synology NAS or laptop should be mindful of system resources (CPU, memory) to allow for other potential uses of the hardware, although dedicated processing during archival tasks is acceptable.

**2\. Reliability & Robustness:** 2.1. **Data Completeness & Integrity:** The archival process must aim for 100% capture of all identified topic pages. Mechanisms should be in place to verify or report on the completeness of the downloaded HTML archive (e.g., checking against the master index of URLs). 2.2. **Error Handling:** Scripts must implement robust error handling for common issues such as network timeouts, HTTP errors from the source server (e.g., temporary unavailability), or unexpected HTML structures during parsing. Errors should be logged clearly, and the script should attempt to retry or skip problematic items gracefully without halting the entire process where feasible. 2.3. **Resumability:** The archival script must be resumable, as defined in the functional requirements, ensuring that long-running jobs can be stopped and restarted without loss of previously archived data. 2.4. **Predictable Behavior:** Scripts should behave consistently and predictably given the same inputs and configurations.

**3\. Usability (of the Archival Scripts & Process):** 3.1. **Configuration:** Key parameters for the scripts (e.g., politeness delay, target directories for NAS, sub-forum IDs to process, off-peak hour definitions) should be easily configurable (e.g., via a configuration file or command-line arguments). 3.2. **Logging Clarity:** Logs generated by the scripts (as defined in functional requirements) must be clear, human-readable, and provide sufficient detail to understand the script's progress, any errors encountered, and the generated metrics (e.g., processing rate, ETCs). 3.3. **Operational Simplicity:** The process for initiating, monitoring, stopping, and resuming the indexing and archival scripts should be straightforward for the user.

**4\. Maintainability (of the Archival Scripts):** 4.1. **Code Clarity:** The script code (Python or other chosen language) should be well-organized, commented where necessary, and follow good programming practices to allow for future understanding, modification, and debugging by the user.

**5\. Scalability (of the Archival Process & Data Store):** 5.1. **Full Forum Volume:** The entire archival process (indexing, HTML download, structured data extraction) must be designed to handle the full anticipated volume of The Magic Cafe forum (estimated 5GB+ of raw text, potentially millions of posts and tens of thousands of threads). 5.2. **Storage Management:** The directory structure and file naming conventions for the raw HTML archive and structured JSON data on the Synology NAS should be designed to manage a large number of files efficiently.

## **User Interaction and Design Goals**

The "Waypoint Archive" phase of this project is primarily focused on data acquisition and backend processing. The scripts and tools developed for this phase will be command-line operated or have minimal operational interfaces designed for the user (the project initiator) to manage the archival process. Standard usability for such tools (clear logging, configurable parameters, straightforward execution) is covered under Non-Functional Requirements.

Detailed User Interaction and Design Goals for the subsequent AI-powered front-end research tool (the ultimate vision of Project Waypoint) will be defined in a later stage of product development, after the "Waypoint Archive" is successfully established. The vision for this tool includes a natural language query interface and conversational interaction, with a focus on intuitive access to the archived data and clear citation of sources.

## **Technical Assumptions**

This section outlines the foundational technical assumptions for Project Waypoint, with an initial focus on the "Waypoint Archive" phase.

**For the "Waypoint Archive" Phase (MVP):**

* **Primary Development Language:** The initial proof-of-concept for thread scraping was developed in Python. While Python is a strong candidate for the archival scripts, the project is open to using other languages if they offer significant performance advantages for text processing and web scraping tasks.  
* **Execution Environment:**  
  * Indexing scripts: To be run on the user's laptop.  
  * HTML archival scripts: To be run on the user's Synology NAS (for continuous, scheduled operation).  
  * Structured data extraction scripts: To be run on the user's Synology NAS.  
* **Data Storage:**  
  * Raw HTML Archive: Stored as individual HTML files in an organized directory structure on the user's Synology NAS.  
  * Structured Data: Stored as JSON files (e.g., one per thread) on the user's Synology NAS.  
  * Topic Index / Progress Files: Stored as flat files (e.g., text, JSON) on the user's laptop (for indexing) and Synology NAS (for archival process state).  
* **Scraping Methodology:** Will adhere to "polite scraping" principles (rate limiting, off-peak execution, custom User-Agent) as defined in previous sections.  
* **Target Forum Stability:** The Magic Cafe forum's HTML structure is assumed to be relatively stable due to its age, facilitating reliable parsing. However, scripts must include robust error handling for unexpected structures.  
* **Network Connectivity:** The user's laptop and Synology NAS are assumed to have reliable internet connectivity for the duration of the indexing and scraping tasks.

**Repository & Service Architecture:**

* **For "Waypoint Archive" Scripts:**  
  * **Repository Structure:** Likely a single Git repository containing the suite of scripts (indexer, archiver, data extractor) and any supporting configuration files or utilities.  
  * **Service Architecture:** N/A in a microservices sense. The system will be a set of interconnected local scripts designed for sequential and/or scheduled execution.  
* **For the Future AI Research Tool (Project Waypoint \- Phase 3 & 4):**  
  * **Repository Structure:** To be determined (TBD). Could be part of the same repository or a new one, depending on the chosen front-end/back-end technologies.  
  * **Service Architecture:** TBD. This will depend on choices regarding the vector database, LLM interaction layer, and the front-end application. Options could range from a monolithic local application to a more distributed setup if deployed for wider use later.

### **Testing requirements**

**For "Waypoint Archive" Scripts (MVP):**

* **Unit Testing:** Focus on testing individual functions, especially HTML parsing logic for extracting specific data elements (e.g., topic IDs, post metadata, quote blocks) using sample HTML snippets.  
* **Integration Testing (Conceptual):** Testing the workflow between scripts (e.g., ensuring the archiver correctly consumes the indexer's output). This will likely be done through controlled test runs on small sub-forums.  
* **Verification:**  
  * Manual and/or scripted verification of data completeness for test sub-forums (e.g., ensuring all expected topic IDs are indexed, all pages are downloaded).  
  * Verification that "polite scraping" mechanisms (delays, User-Agent) are functioning as intended.  
  * Testing the resumability of the archival script.  
  * Verification of the structured data output against known HTML inputs.  
* **Logging and Metrics Validation:** Ensuring the logging outputs and ETC calculations are accurate and useful.

**For the Future AI Research Tool (Project Waypoint \- Phase 3 & 4):**

* Detailed testing requirements (including unit, integration, E2E, and performance testing for the AI components and user interface) will be defined when that phase of the project is planned.

## **Epic Overview**

**Epic 1: Forum Indexing System Development & Initial Full Index Creation**

* **Goal:** To develop, test, and execute a robust indexing system capable of navigating all sub-forums and their pages on The Magic Cafe, generating a comprehensive and de-duplicated master list of all unique topic IDs and their URLs, preparing the groundwork for complete content archival.

   **Story 1.1: Develop Sub-Forum Page Navigation Logic**

  1. **As a** Developer of the Indexing System,  
  2. **I want** to create a module that can reliably identify all unique pagination links for a given Magic Cafe sub-forum's thread listing pages (e.g., `viewforum.php?forum=X`) and generate an ordered sequence of all page URLs for that sub-forum,  
  3. **So that** the Indexing System can systematically visit every page where individual topic/thread URLs are listed, ensuring no page of threads is missed. **Acceptance Criteria (ACs):**  
  4. **AC1:** Given the base URL of any Magic Cafe sub-forum (e.g., `https://www.themagiccafe.com/forums/viewforum.php?forum=X`), the system can successfully fetch and parse its HTML content.  
  5. **AC2:** The system can correctly identify the HTML elements containing pagination links on a sub-forum's thread listing page (e.g., links for "Next", "Previous", and specific page numbers).  
  6. **AC3:** If a "Next Page" link exists, the system can extract its URL.  
  7. **AC4:** If numbered page links exist (e.g., "1, 2, 3, ..., Last"), the system can extract the URL for each explicitly listed page number.  
  8. **AC5:** The system can determine the URL for the last page of thread listings, either by finding a "Last Page" link or by deducing it from the highest numbered page link present.  
  9. **AC6:** The system can generate a complete, ordered list of all unique URLs for every page of thread listings within that sub-forum, from page 1 to the last page.  
  10. **AC7:** The logic correctly handles sub-forums that have only a single page of threads (i.e., no pagination links are present beyond the current page).  
  11. **AC8:** The logic correctly handles sub-forums with multiple pages of threads, including scenarios where pagination might involve ellipses (e.g., "1, 2, ..., 10, 11").  
  12. **AC9:** The system logs an informative message or error if it encounters an unexpected pagination structure it cannot parse on a sub-forum page.  
* **Story 1.2: Develop Topic ID & URL Extraction Logic**

  1. **As a** Developer of the Indexing System,  
  2. **I want** to create a module that can accurately parse the HTML content of a single Magic Cafe sub-forum page (a page listing multiple topics) to identify and extract the title, unique Topic ID, and direct URL for each individual topic listed on that page,  
  3. **So that** a comprehensive list of all topics to be archived can be systematically compiled. **Acceptance Criteria (ACs):**  
  4. **AC1:** Given the raw HTML content of a single thread listing page from a Magic Cafe sub-forum (e.g., a `viewforum.php?forum=X&start=Y` page), the system can successfully identify all distinct HTML elements that represent individual topics (threads).  
  5. **AC2:** For each identified topic element, the system can accurately extract the full topic title as displayed on the page.  
  6. **AC3:** For each identified topic element, the system can accurately extract its unique Topic ID (this is often part of the topic's URL, e.g., the `topic=N` parameter).  
  7. **AC4:** For each identified topic element, the system can accurately extract the direct URL that links to that specific topic (e.g., a `viewtopic.php?topic=N...`) if readily available and distinct from a constructible URL based on Topic ID alone.  
  8. **AC5:** The system correctly associates the extracted title and Topic ID (and readily available URL, if applicable) for each topic.  
  9. **AC6:** The system can process pages containing a variable number of topics, including the last page of a sub-forum which may not be full.  
  10. **AC7:** The system correctly distinguishes and extracts data for regular topics, "sticky" topics, and any "announcement" topics if they have a similar linkable structure leading to thread content.  
  11. **AC8:** The system logs an informative message or error if it encounters an HTML structure for a topic element that it cannot parse as expected, and ideally, skips that problematic element while continuing with others if possible.  
  12. **AC9:** The list of extracted topic data (primarily Topic IDs, plus titles/URLs if captured) from a single processed page does not contain duplicates originating from that same page.  
* **Story 1.3 (Revised): Implement Two-Pass Indexing Strategy for Sub-Forums**

  1. **As a** Developer of the Indexing System,  
  2. **I want** to implement a two-pass processing logic when indexing any given sub-forum. This involves performing an initial full scan of all its thread listing pages, followed immediately by a targeted re-scan of only its first thread listing page,  
  3. **So that** I can accurately capture **unique Topic IDs** (and any readily available associated metadata like titles or directly linked URLs) for topics that may have been reordered to the front page or newly added due to recent replies that occurred during the initial full scan of that sub-forum, thus maximizing data completeness for subsequent archival. **Acceptance Criteria (ACs):**  
  4. **AC1:** For a designated target sub-forum, the system MUST first execute a complete scan of all its thread listing pages (using the navigation logic from Story 1.1) and collect all encountered **unique Topic IDs** (and any readily available associated titles/URLs, as per Story 1.2 logic) into a preliminary, de-duplicated list for that sub-forum.  
  5. **AC2:** Immediately upon completion of the full scan (AC1) for that specific sub-forum, the system MUST automatically re-fetch and re-parse the HTML content of *only the first page* of that same sub-forum's thread listings.  
  6. **AC3:** The system MUST extract all **Topic IDs** (and any associated titles/URLs) from this re-scanned first page using the logic from Story 1.2.  
  7. **AC4:** The system MUST compare the **Topic IDs** obtained from the re-scanned first page (AC3) against the preliminary list of **Topic IDs** gathered during the full scan (AC1).  
  8. **AC5:** Any **Topic IDs** identified in AC4 as present on the re-scanned first page but *not* in the preliminary list MUST be added to the final master list of **Topic IDs** for that sub-forum.  
  9. **AC6:** The final compiled list of **Topic IDs** for the sub-forum MUST be de-duplicated, ensuring each **Topic ID** is present only once.  
  10. **AC7:** The system MUST log the start and completion of the initial full scan phase for the sub-forum.  
  11. **AC8:** The system MUST log the start and completion of the first-page re-scan phase for the sub-forum.  
  12. **AC9:** The system SHOULD log the number of new or "bumped" **Topic IDs** that were identified and added to the master list specifically as a result of the first-page re-scan.  
* **Story 1.4: Implement Persistent Topic Index Storage**

  1. **As a** Developer of the Indexing System,  
  2. **I want** to implement functionality that saves the collected unique Topic IDs (and any readily available associated metadata like titles or directly linked URLs) for each processed sub-forum to a persistent local file, ensuring this list is de-duplicated and easily consumable,  
  3. **So that** this master Topic Index can be reliably used as input by the Archival System (Epic 2), and the indexing process can be managed incrementally or resumed if interrupted. **Acceptance Criteria (ACs):**  
  4. **AC1:** Upon completing the indexing process for a sub-forum (including both passes as defined in Story 1.3), the system MUST save the final, de-duplicated list of unique Topic IDs (along with any associated titles or directly-extracted URLs captured by Story 1.2 logic) to a local file.  
  5. **AC2:** The storage format for the index file(s) MUST be simple, machine-readable, and clearly defined (e.g., one Topic ID per line in a TXT file, or a structured format like JSON or CSV).  
  6. **AC3:** The system MUST ensure that only unique Topic IDs are written to the persistent storage for any given sub-forum.  
  7. **AC4:** The system SHOULD allow for a configurable output location (directory path) on the user's laptop for these index files.  
  8. **AC5:** The system's output MAY consist of one index file per processed sub-forum (e.g., `forum_54_index.txt`) OR a consolidated master index file that clearly associates Topic IDs with their respective sub-forums. The chosen structure must be consistent.  
  9. **AC6:** If the indexing process for a sub-forum is re-run, the system SHOULD offer a choice or have a defined behavior (e.g., overwrite, append new unique IDs) for handling existing index files for that sub-forum to prevent data loss or excessive duplication, based on configuration.  
  10. **AC7:** The system MUST log a confirmation message upon successfully saving the Topic Index for a sub-forum, including the file path and the number of unique Topic IDs saved.  
  11. **AC8:** The format of the saved Topic Index file(s) MUST be directly and easily parsable by the Archival System to be developed in Epic 2\.  
* **Story 1.5: Implement Indexing Performance Metrics & ETC Calculation**

  1. **As an** Operator of the Indexing System,  
  2. **I want** the system to actively track its performance metrics (such as indexing rate) in real-time and use this data to provide dynamically updated Estimated Time to Completion (ETC) for the current sub-forum indexing task, as well as to generate initial ETCs for future, unstarted sub-forums based on historical performance,  
  3. **So that** I can effectively monitor the progress of long-running indexing tasks, manage my expectations regarding timelines, and better plan the overall forum indexing effort. **Acceptance Criteria (ACs):**  
  4. **AC1:** The Indexing System MUST continuously track its processing rate during operation (e.g., number of sub-forum pages processed per minute, and/or number of unique Topic IDs collected per minute).  
  5. **AC2:** While indexing a sub-forum, the system MUST display and regularly update (e.g., every few minutes or after processing a set number of pages) an Estimated Time to Completion (ETC) for the *current* sub-forum.  
  6. **AC3:** The ETC for the current sub-forum MUST be calculated based on the system's real-time measured processing rate for that sub-forum and the remaining number of known pages (or estimated topics) for that sub-forum.  
  7. **AC4:** The accuracy of the dynamically updated ETC for the current sub-forum SHOULD improve as more of that sub-forum is processed.  
  8. **AC5:** Upon completion of indexing a sub-forum, the system MUST log key performance metrics for that run, such as the total time taken, average processing rate, and total items processed.  
  9. **AC6:** This historical performance data (e.g., average processing rates from completed sub-forum runs) MUST be stored persistently in a simple local format (e.g., a log file, a small database, or a structured text file).  
  10. **AC7:** Before initiating the indexing of a *new* (unstarted) sub-forum, the system (or an associated utility function) MUST be able to provide an initial ETC. This estimate should be based on the known size of the new sub-forum (e.g., total estimated number of pages, from Story 1.1's output) and the stored historical average performance data.  
  11. **AC8:** All displayed ETCs and logged processing rates MUST be presented in a clear, human-readable format (e.g., "ETC: \~2 hours 30 minutes", "Rate: 15 pages/minute").  
  12. **AC9:** The collection and calculation of performance metrics and ETCs MUST NOT significantly degrade the primary performance or stability of the indexing tasks themselves.  
* **Story 1.6: Integrate and Configure Core Indexing Script**

  1. **As a** Developer/Operator of the Indexing System,  
  2. **I want** to integrate all the developed indexing modules—including sub-forum page navigation, topic ID (and ancillary data) extraction, the two-pass indexing strategy, persistent Topic Index storage, and performance metrics/ETC calculation—into a single, configurable, and executable core indexing script,  
  3. **So that** I can reliably and efficiently initiate and manage the complete indexing process for any target Magic Cafe sub-forum from my laptop, producing the required persistent Topic Index. **Acceptance Criteria (ACs):**  
  4. **AC1:** A primary executable script (e.g., a main Python script) is created that orchestrates the end-to-end indexing process for a specified Magic Cafe sub-forum.  
  5. **AC2:** The script successfully incorporates and utilizes the sub-forum page navigation logic (developed in Story 1.1) to iterate through all thread listing pages of a target sub-forum.  
  6. **AC3:** The script successfully incorporates and utilizes the topic ID and ancillary data (e.g., title, readily available URL) extraction logic (developed in Story 1.2) for each topic found on those pages.  
  7. **AC4:** The script correctly implements and executes the two-pass indexing strategy (initial full scan followed by a first-page re-scan) for the target sub-forum, as defined in Story 1.3.  
  8. **AC5:** The script correctly utilizes the persistent Topic Index storage mechanisms (developed in Story 1.4) to save the final, de-duplicated list of unique Topic IDs (and any associated data) for the processed sub-forum to a local file.  
  9. **AC6:** The script accurately implements, tracks, and displays/logs the performance metrics and Estimated Time to Completion (ETC) calculations as defined in Story 1.5.  
  10. **AC7:** The core indexing script MUST be configurable, allowing the user to specify (e.g., via command-line arguments or a simple configuration file):  
      * The target sub-forum ID (or its base URL).  
      * The output directory for the persistent Topic Index file(s).  
      * Key "polite scraping" parameters (e.g., the delay between HTTP requests).  
      * Logging verbosity level (e.g., DEBUG, INFO, ERROR).  
  11. **AC8:** The script MUST produce comprehensive operational logs detailing its progress, key actions, performance metrics, ETCs, and any errors encountered, consistent with the logging requirements from previous stories.  
  12. **AC9:** The script MUST be runnable and perform as expected within the user's laptop environment (assuming all necessary dependencies, like Python and required libraries, are correctly installed).  
  13. **AC10:** Upon successful completion of an indexing run for a sub-forum, the script generates the expected persistent Topic Index file(s) in the specified location, containing the accurate and de-duplicated set of Topic IDs for that sub-forum.  
* **Story 1.7: Conduct Test Run & Refine Indexing System on a Small Sub-Forum**

  1. **As a** Developer/Operator of the Indexing System,  
  2. **I want** to execute the fully integrated Core Indexing Script (developed in Story 1.6) on a designated small, manageable sub-forum (e.g., the one with \~2000 topics, `viewforum.php?forum=54`) and meticulously analyze its performance, the accuracy of its output, and the utility of its logging and metrics,  
  3. **So that** I can validate the entire indexing system's functionality in a real-world scenario, gather initial performance benchmarks, and identify any necessary refinements or bugs before attempting larger-scale or full-forum indexing tasks. **Acceptance Criteria (ACs):**  
  4. **AC1:** A specific, relatively small Magic Cafe sub-forum (e.g., `forum=54`) is formally selected for the initial test run.  
  5. **AC2:** The Core Indexing Script (from Story 1.6) is configured with the target test sub-forum, appropriate politeness settings (e.g., initial delay of 3-5 seconds), and output locations for logs and the Topic Index file.  
  6. **AC3:** The script is executed on the user's laptop and runs the complete two-pass indexing process for the test sub-forum from start to finish.  
  7. **AC4:** The script completes the test run without unhandled exceptions or critical errors that prematurely halt the entire process. (Gracefully handled errors, if any, should be logged).  
  8. **AC5:** A persistent Topic Index file for the test sub-forum is successfully generated in the specified output location.  
  9. **AC6:** The content of the generated Topic Index file is reviewed:  
     * The number of unique Topic IDs collected is manually compared against an estimate for the test sub-forum to ensure reasonable completeness.  
     * A spot-check of a few Topic IDs and any associated data (like titles, if captured) is performed to verify accuracy.  
  10. **AC7:** The performance metrics (e.g., processing rate, total time taken) and the dynamically calculated Estimated Time to Completion (ETC) logged by the script during the test run are reviewed for plausibility and consistency.  
  11. **AC8:** The operational logs generated during the test run are reviewed for clarity, informativeness, and to identify any unexpected warnings or non-critical errors.  
  12. **AC9:** Based on the outcomes of the test run (AC3-AC8), initial real-world performance benchmarks (e.g., average pages/topics indexed per minute with the chosen politeness delay for *this specific forum's structure*) are documented.  
  13. **AC10:** Any bugs, significant performance bottlenecks, inaccuracies in data extraction, or deficiencies in logging/metrics identified during the test run are documented, and a plan for their refinement (either immediate or before scaling up) is noted.  
* **Story 1.8: Execute Indexing for Prioritized Large Sub-Forums**

  1. **As an** Operator of the Indexing System,  
  2. **I want** to execute the validated and refined Core Indexing Script (from Story 1.7) on one or more pre-identified, high-value, large sub-forums (such as the "Coins" sub-forum),  
  3. **So that** I can build a substantial portion of the master Topic Index for Project Waypoint and further verify the script's stability, performance, and metric accuracy when handling larger volumes of data and longer runtimes. **Acceptance Criteria (ACs):**  
  4. **AC1:** At least one specific large, high-value sub-forum (e.g., the "Coins" sub-forum, with its \~11,000 topics) is formally designated as the target for this execution run. Additional prioritized sub-forums may also be included in the scope of this story.  
  5. **AC2:** The Core Indexing Script, incorporating any refinements from Story 1.7, is configured for the target large sub-forum(s), including appropriate politeness settings and output locations.  
  6. **AC3:** The script is executed and successfully runs the complete two-pass indexing process for each targeted large sub-forum from start to finish.  
  7. **AC4:** The script completes these potentially long-duration runs without unhandled exceptions or critical errors that prematurely halt the process for a given sub-forum. Any non-critical errors are logged appropriately.  
  8. **AC5:** A persistent Topic Index file is successfully generated and saved in the specified output location for each targeted large sub-forum.  
  9. **AC6:** The number of unique Topic IDs collected for each processed large sub-forum is logged and reviewed for consistency with the known size of that sub-forum.  
  10. **AC7:** Performance metrics (e.g., overall processing rate, total time taken per large sub-forum) and the behavior of the Estimated Time to Completion (ETC) during these longer runs are logged and analyzed.  
  11. **AC8:** The system demonstrates stability and effective resource management (on the user's laptop) throughout the extended duration of indexing these larger sub-forums.  
  12. **AC9:** The operational logs for these runs are reviewed to identify any new issues, warnings, or performance characteristics that may only become apparent when processing data at this larger scale.  
  13. **AC10:** The Topic Index files generated from these runs are confirmed to be correctly formatted and represent a significant addition to the master list of Topic IDs required for the "Waypoint Archive."  
* **Story 1.9: Complete Full Forum Index Generation**

  1. **As an** Operator of the Indexing System,  
  2. **I want** to systematically execute the validated and refined Core Indexing Script on all remaining Magic Cafe sub-forums that have not yet been processed (i.e., those not covered in Story 1.7 or Story 1.8),  
  3. **So that** I can produce a final, comprehensive, and de-duplicated master list of all unique Topic IDs for the entire Magic Cafe forum, thereby completing the indexing phase of the "Waypoint Archive." **Acceptance Criteria (ACs):**  
  4. **AC1:** A definitive list of all sub-forums on The Magic Cafe is obtained or created.  
  5. **AC2:** A mechanism or process is in place to identify and track which sub-forums from the definitive list (AC1) have already been successfully indexed in previous stories (e.g., Story 1.7, Story 1.8) and which ones remain.  
  6. **AC3:** The Core Indexing Script is systematically executed for all remaining, un-indexed sub-forums, either one by one or in manageable batches.  
  7. **AC4:** The script successfully runs its complete two-pass indexing process for each of these remaining sub-forums, continuing to adhere to "polite scraping" best practices.  
  8. **AC5:** Persistent Topic Index data (either as individual files per sub-forum or by updating a consolidated master index) is correctly generated and saved for each newly processed sub-forum.  
  9. **AC6:** The system continues to log performance metrics and operational details for these runs. Any significant deviations in performance or new types of errors encountered on previously untested sub-forums are noted.  
  10. **AC7:** Upon successful processing of all identified sub-forums, a final, consolidated master list containing all unique Topic IDs from the entire Magic Cafe forum is assembled and verified for de-duplication.  
  11. **AC8:** The total count of unique Topic IDs in the final master list is recorded, serving as a key metric for the scope of the "Waypoint Archive."  
  12. **AC9:** The process for identifying remaining sub-forums and systematically completing their indexing is documented.

**Epic 2: Raw HTML Archival System Development & Full Archive Execution**

* **Goal:** To develop, test, and execute a polite, resumable archival system that downloads and stores the complete raw HTML content of every page for every topic identified in Epic 1, creating a permanent local backup of The Magic Cafe forum on the user's Synology NAS. *(User Stories for Epic 2 to be detailed later)*

**Epic 3: Structured Data Extraction System Development & Initial Processing**

* **Goal:** To develop, test, and execute a system that processes the archived raw HTML, accurately extracting post metadata and content (including structured quote information) into a locally stored, machine-readable format (e.g., JSON files per thread), making the data ready for future AI processing. *(User Stories for Epic 3 to be detailed later)*

## **Key Reference Documents**

*(To be populated as needed)*

## **Out of Scope Ideas Post MVP**

*(This refers to ideas out of scope for the overall Project Waypoint AI tool, beyond the "Waypoint Archive" MVP. To be populated later if specific ideas arise that are deferred beyond the planned AI tool features.)*

* The "Waypoint Archive" MVP explicitly defers text embedding, vector database creation, and LLM front-end development to subsequent project phases.

## **\[OPTIONAL: For Simplified PM-to-Development Workflow Only\] Core Technical Decisions & Application Structure**

*(This section is not applicable as Project Waypoint will involve architectural design for the AI tool phase.)*

## **Change Log**

| Change | Date | Version | Description | Author |
| ----- | ----- | ----- | ----- | ----- |
|  |  |  |  |  |

Export to Sheets

\----- END PRD START CHECKLIST OUTPUT \------

*(Checklist not run yet)*

\----- END Checklist START Design Architect `UI/UX Specification Mode` Prompt \------

*(Not applicable for "Waypoint Archive" MVP)*

\----- END Design Architect `UI/UX Specification Mode` Prompt START Architect Prompt \------

## **Initial Architect Prompt**

*(This prompt is primarily for the later AI research tool phase of Project Waypoint. For the "Waypoint Archive" MVP, architectural decisions are simpler and embedded within the technical assumptions and functional requirements of the scripts.)*

Based on our discussions and requirements analysis for **Project Waypoint**, particularly for the AI research tool to be built upon the **"Waypoint Archive,"** the following initial technical guidance will inform your architecture analysis and decisions when that phase begins:

### **Technical Infrastructure (for the future AI Tool)**

* **Data Source:** The "Waypoint Archive" (locally stored raw HTML and structured JSON data).  
* **Core Functionality:** Semantic search, natural language querying, conversational interaction with archived forum content.  
* **Key Technologies to Incorporate (User is willing to pay API costs):**  
  * Text Embedding Models (e.g., OpenAI)  
  * Vector Database (specific choice TBD, could be local or cloud-based depending on scale and budget for this phase)  
  * Large Language Models (e.g., OpenAI GPT series) for response synthesis and interaction.  
* **Deployment Environment for AI Tool:** TBD (could range from a local application to a cloud-hosted service, depending on user's goals post-archival).

### **Technical Constraints (for the future AI Tool)**

* Must interface with the structured JSON output of the "Waypoint Archive."  
* Must provide accurate citations linking back to the raw HTML archive.  
* Development will occur after the "Waypoint Archive" is complete.

### **Local Development & Testing Requirements (for the future AI Tool)**

* To be defined, but will likely involve working with subsets of the embedded data for testing query relevance and LLM interaction.

### **Other Technical Considerations (for the future AI Tool)**

* Scalability of the vector search solution if the archive is very large.  
* Cost management for embedding and LLM API calls during development and operation.  
* Potential for a simple web front-end for user interaction.

This information should serve as a starting point for the architectural design of the AI-powered research tool component of Project Waypoint once the "Waypoint Archive" foundational data is secured.

\----- END Architect Prompt \-----
