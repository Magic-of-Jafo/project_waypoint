# Epic 2: Raw HTML Archival System Development & Full Archive Execution

**Goal:** To develop, test, and execute a polite, resumable archival system that downloads and stores the complete raw HTML content of every page for every topic identified in Epic 1, creating a permanent local backup of The Magic Cafe forum on the user's Synology NAS.

---

## User Stories

**Story 2.1 (Revised): Develop Topic Index Consumption and Prioritization Logic**

* **As the** Archival System,
* **I want** to reliably read and parse the persistent Topic Index file(s) (generated by the Indexing System in Epic 1), determine the total number of topics for each sub-forum, and then prepare a master list of all individual Topic IDs **ordered by sub-forum size (ascending, from fewest topics to most)**,
* **So that** the HTML archival process can be executed systematically, starting with smaller sub-forums, and I have a complete and accurately prioritized list of all topics to be archived.

**Acceptance Criteria (ACs):**

1.  **AC1:** The system MUST be configurable with the file path(s) or directory where the Topic Index file(s) (generated by Epic 1) are stored.
2.  **AC2:** The system MUST successfully read and parse the defined format of the Topic Index file(s) to extract individual Topic ID entries and their associated sub-forum identifiers.
3.  **AC3:** The system MUST be able to process the Topic Index data to accurately determine the total count of unique Topic IDs for each distinct sub-forum.
4.  **AC4:** The system MUST generate an ordered list of sub-forums, sorted by their total topic count in ascending order (smallest sub-forum first).
5.  **AC5:** For each entry in the Topic Index, the system MUST extract the unique Topic ID. If the Topic Index entries contain additional readily available metadata (such as topic titles or directly scraped URLs), this associated metadata MUST also be correctly parsed and retained alongside each Topic ID.
6.  **AC6:** The system MUST consolidate all Topic IDs from all sub-forums into a single internal master list (or a structure that allows iteration through all topics), **and this list MUST be structured or iterated in the order determined by AC4 (smallest sub-forum's topics first, then next smallest, etc.).**
7.  **AC7:** The system MUST ensure that the final list of Topic IDs to be processed for archival is de-duplicated globally across all sub-forums.
8.  **AC8:** The system MUST log the total number of unique Topic IDs successfully loaded and the determined processing order of sub-forums (e.g., "Will process sub-forum X (50 topics), then sub-forum Y (120 topics)...").
9.  **AC9:** The system MUST log a clear error message and halt gracefully if the Topic Index file(s) cannot be found or are in an unparsable format.
10. **AC10:** The loaded, parsed, and **ordered** list of Topic IDs (with any associated metadata) MUST be available in a usable data structure for subsequent modules of the Archival System.

---

**Story 2.3: Implement Raw HTML Page Download Functionality**

* **As the** Archival System,
* **I want** to take any specific Magic Cafe topic page URL (provided by the Intra-Topic Page Navigation logic from Story 2.2) and reliably download its complete, unaltered, raw HTML content,
* **So that** this raw HTML can be passed to the file storage module (Story 2.4) for preservation in the "Waypoint Archive."

**Acceptance Criteria (ACs):**

1.  **AC1:** Given a valid URL for an individual page within a Magic Cafe topic, the system MUST be able to successfully execute an HTTP GET request to that URL.
2.  **AC2:** The system MUST retrieve the full HTTP response from the server for the given URL.
3.  **AC3:** The system MUST extract the complete, raw HTML content (as a byte stream or accurately decoded string, ensuring original encoding is preserved or handled correctly, typically UTF-8 for modern web content but should be verified from response headers) from the body of the HTTP response.
4.  **AC4:** The downloaded HTML content MUST be preserved exactly as received from the server. No parsing (beyond what's necessary to get the raw content from the response object), modification, or stripping of tags should occur during this download process itself (full parsing happens in Epic 3).
5.  **AC5:** The system MUST correctly handle character encodings as specified in the HTTP response headers. If no encoding is specified, it should default to a sensible standard (e.g., UTF-8) and ensure the raw byte stream is captured faithfully if precise decoding is uncertain, to prevent data loss.
6.  **AC6:** The system MUST implement robust error handling for network-related issues that may occur during the download attempt (e.g., connection timeouts, DNS resolution failures, SSL handshake issues), adhering to the protocols defined in `docs/operational-guidelines.md` (Section 4: Error Handling and Logging Protocols). This includes logging the error and the problematic URL.
7.  **AC7:** The system MUST handle HTTP error status codes returned by the server (e.g., 403 Forbidden, 404 Not Found, 5xx Server Errors) by:
    * Logging the problematic URL and the received HTTP status code.
    * Following the defined retry strategy (if applicable for the error type, as per Story 2.5 and `docs/operational-guidelines.md`) or gracefully skipping the page and marking it as problematic for this run.
8.  **AC8:** The "polite scraping" mechanisms (such as configured delays *before* making the request and sending the custom User-Agent string, as defined in Story 2.5) MUST be respected by this download functionality.
    *   ***Note for Scrum Master/Dev for Story 2.3:*** *Ensure that the `PolitenessDelay` configured in `pkg/config/config.go` (and intended to be populated from CLI flags/Story 2.5) is correctly passed to and utilized by the HTML fetching mechanism used in this story.* 
9.  **AC9:** The successfully downloaded raw HTML content for a page MUST be made available as output (e.g., as a string or byte slice/array in Go) to be consumed by the next module responsible for file storage (Story 2.4).

---

**Story 2.4: Implement HTML File Storage & Organization on NAS**

* **As the** Archival System,
* **I want** to take the downloaded raw HTML content for each specific topic page and save this content as a distinct file to a specified local storage location (e.g., the user's Synology NAS), ensuring files are placed within a clearly defined and organized directory structure,
* **So that** the raw HTML of every archived page is stored persistently, safely, and can be easily located for future processing (by Epic 3) or direct reference.

**Acceptance Criteria (ACs):**

1.  **AC1:** Given the raw HTML content (as a byte stream or string, output from Story 2.3) for a specific topic page, along with its identifying metadata (e.g., sub-forum identifier, Topic ID, page number), the system MUST be able to save this content into a new file.
2.  **AC2:** Files MUST be saved to a user-configurable root directory path. This path should be platform-agnostic to support execution on different environments (e.g., laptop or Docker container mapping to NAS storage).
3.  **AC3:** The system MUST create and use the agreed-upon logical directory structure for storing HTML files, for example: `{ARCHIVE_ROOT}/{sub_forum_id_or_name}/{topic_id}/page_{page_number}.html` (as per Functional Requirement 2.5).
4.  **AC4:** The system MUST automatically create any necessary sub-directories within the archive path if they do not already exist (e.g., creating the `{sub_forum_id_or_name}` and `{topic_id}` directories).
5.  **AC5:** The system MUST handle file operations correctly: if a file for a specific page is re-downloaded (e.g., during a resumability test or a deliberate re-run for that page), the system should overwrite the existing file for that specific page to ensure the latest version is stored.
6.  **AC6:** All file write operations MUST include robust error handling (e.g., for "disk full," "permission denied," "invalid path" scenarios) and log such errors clearly, adhering to the protocols defined in `docs/operational-guidelines.md`.
7.  **AC7:** Upon successfully saving an HTML file, the system SHOULD log a confirmation message, including the full file path and ideally the size of the file saved.
8.  **AC8:** The content of the saved HTML files MUST be an exact match to the raw HTML content downloaded in Story 2.3, with no alteration, corruption, or unintended re-encoding occurring during the save process.
9.  **AC9:** File and directory naming MUST be consistent and adhere to common operating system limitations (e.g., regarding special characters, path lengths), ensuring portability and accessibility of the archive.

---

**Story 2.5: Implement and Tune "Polite Scraping" Mechanisms**

* **As the** Archival System,
* **I want** to incorporate robust and highly configurable "polite scraping" mechanisms, including adjustable request delays, a custom User-Agent, and the inherent capability to be run during defined off-peak hours,
* **So that** I minimize my operational impact on The Magic Cafe server, reduce the risk of being IP banned or overwhelming the server, and allow the operator (the user) to easily test, monitor, and tune these settings to find an optimal balance between archival speed and server consideration.

**Acceptance Criteria (ACs):**

1.  **AC1:** The system MUST implement a configurable delay mechanism that enforces a pause (e.g., specified in seconds or milliseconds) between each individual HTTP request made to The Magic Cafe server during the archival process.
2.  **AC2:** The delay duration MUST be easily configurable by the user at runtime (e.g., via a command-line argument or a central configuration file for the archival script). A sensible default delay (e.g., 3 to 5 seconds) MUST be implemented if no specific delay is configured.
3.  **AC3:** The system MUST send a custom User-Agent string with every HTTP request. This User-Agent string MUST be configurable by the user, with a default value as specified in `docs/operational-guidelines.md` (Section 5.5).
4.  **AC4:** The archival script MUST be designed to be cleanly startable and stoppable by an external scheduler (like `cron` on the Synology NAS), which facilitates its execution primarily during user-defined off-peak hours. The script itself does not need to implement the scheduling logic but must support it.
5.  **AC5:** The currently active politeness settings (e.g., request delay interval, User-Agent being used) MUST be clearly logged by the script at the beginning of each execution run.
6.  **AC6:** During test runs (specifically as part of Story 2.9), the operator MUST be able to easily:
    * Modify the request delay setting.
    * Execute the script (or portions of it) with different delay settings.
    * Observe the impact of these changes on both the archival speed (using metrics from Story 2.7) and server responsiveness (by monitoring HTTP status codes and error logs for signs of server strain, such as HTTP 429 "Too Many Requests" or 503 "Service Unavailable" errors).
7.  **AC7:** If the configured politeness delay is significant (e.g., 1 second or more), the script's logging SHOULD clearly indicate when it is actively pausing due to this delay, making its behavior transparent during long runs.
8.  **AC8:** The implementation of these politeness mechanisms MUST be efficient and not add undue performance overhead to the script itself, beyond the intended, configurable delays.

---

**Story 2.6: Implement Resumable State Management for Archival**

* **As the** Archival System,
* **I want** to maintain a persistent record of my progress through the master list of Topic IDs (and the pages within each topic) that are being archived, updating this record frequently and reliably,
* **So that** if the archival process is stopped or interrupted for any reason (e.g., manual stop, script error, system shutdown, end of a scheduled execution window), it can be restarted and will automatically and accurately resume from the exact point it left off, preventing the re-downloading of already archived pages and ensuring no content is inadvertently skipped.

**Acceptance Criteria (ACs):**

1.  **AC1:** The system MUST maintain a persistent state that accurately records, at a minimum, the last successfully and completely archived Topic ID and, if a topic was only partially completed during the interruption, the last successfully archived page number *within* that topic.
2.  **AC2:** This progress state MUST be saved to a designated local file (e.g., `archive_progress.json`, `last_processed_state.txt`) in a user-configurable location (e.g., on the Synology NAS alongside the archive or logs).
3.  **AC3:** The progress state file MUST be updated at regular, defined intervals during the archival process (e.g., after each topic is fully archived, or after every N successfully archived pages, with N being configurable but defaulting to a reasonably small number to minimize reprocessing on resume).
4.  **AC4:** Upon starting an archival run, the system MUST automatically check for the existence of this progress state file.
5.  **AC5:** If a progress state file exists and indicates a previously interrupted run, the system MUST automatically resume the archival process from the next logical item based on the saved state:
    * If a topic was partially completed, it resumes with the next unarchived page of that topic.
    * If a topic was fully completed, it resumes with the next Topic ID in the prioritized list (as determined by Story 2.1).
6.  **AC6:** When resuming, the system MUST correctly use the master Topic Index (as loaded and ordered by Story 2.1) to identify and skip all Topic IDs and pages within those topics that were successfully archived before the interruption, based on the progress state file.
7.  **AC7:** The state management logic MUST correctly interact with the prioritized processing order of sub-forums and topics (as defined in Story 2.1) to ensure resumption occurs at the correct point in the overall sequence.
8.  **AC8:** The resumability feature MUST function correctly and be essential for scripts designed to run in scheduled batches (e.g., for off-peak operation), allowing each new batch to seamlessly continue from where the previous batch concluded.
9.  **AC9:** The format of the progress state file MUST be robust, easily parsable by the script, and employ safe-writing techniques (e.g., write to a temporary file then rename, or use journaling) to minimize the risk of corruption if an interruption occurs *during* a state save.
10. **AC10:** In the event of an unexpected crash or ungraceful shutdown, the last successfully saved state should allow the script to resume with minimal reprocessing (i.e., at most, only re-archiving the items processed since the very last successful state save).
11. **AC11:** The system SHOULD log when it is resuming from a previous state and the point from which it is resuming.

---

**Story 2.7: Implement Archival Performance Metrics & ETC Calculation**

* **As an** Operator of the Archival System,
* **I want** the system to actively track its performance metrics (such as pages/threads archived per unit of time, and data volume downloaded) in real-time and use this data to provide dynamically updated Estimated Time to Completion (ETC) for the current archival task or batch, as well as to generate initial ETCs for future archival batches based on historical performance,
* **So that** I can effectively monitor the progress of potentially very long-running archival tasks, manage my expectations regarding timelines, and better plan the overall forum archival effort.

**Acceptance Criteria (ACs):**

1.  **AC1:** The Archival System MUST continuously track its processing rate during operation. Key rates to track include:
    * Number of individual topic pages successfully downloaded and saved per unit of time (e.g., pages per minute).
    * Number of complete topics successfully archived per unit of time (e.g., topics per hour).
    * Volume of data downloaded per unit of time (e.g., MB per minute).
2.  **AC2:** While archiving a batch of topics (e.g., for a specific sub-forum, or the entire remaining list), the system MUST display and regularly update (e.g., every 5-10 minutes or after a significant number of items) an Estimated Time to Completion (ETC) for the *current* operational batch.
3.  **AC3:** The ETC for the current batch MUST be calculated based on the system's real-time measured processing rate(s) and the remaining number of known items (e.g., pages or topics) for that batch.
4.  **AC4:** The accuracy of the dynamically updated ETC for the current batch SHOULD improve as more of that batch is processed and the system "learns" its sustained rate for the current conditions.
5.  **AC5:** Upon completion of archiving a significant batch (e.g., all topics for a sub-forum, or the end of a scheduled operational window), the system MUST log summary performance metrics for that run. This includes total time taken, average processing rates (pages/minute, topics/hour, MB/minute), and total items/volume processed.
6.  **AC6:** This historical performance data (e.g., average processing rates from completed archival runs or batches) MUST be stored persistently in a simple local format (e.g., appended to a performance log file, or a small local database/structured text file).
7.  **AC7:** Before initiating the archival of a *new* batch of topics (e.g., the next sub-forum in the prioritized list), the system (or an associated utility function) MUST be able to provide an initial ETC for that batch. This estimate should be based on the known size of the new batch (e.g., total estimated number of pages derived from the Topic Index) and the stored historical average performance data.
8.  **AC8:** All displayed ETCs and logged processing rates MUST be presented in a clear, human-readable format (e.g., "ETC: ~3 days 4 hours 15 minutes", "Rate: 30 pages/min", "Processed: 1500 of 11000 topics").
9.  **AC9:** The collection and calculation of performance metrics and ETCs MUST NOT significantly degrade the primary performance or stability of the HTML download and storage tasks themselves.

---

**Story 2.8 (Revised): Integrate and Configure Core, Platform-Agnostic Archival Script**

* **As a** Developer/Operator of the Archival System,
* **I want** to integrate all developed archival modules—including topic index consumption and prioritization, intra-topic page navigation, raw HTML page download, HTML file storage, polite scraping mechanisms, resumable state management, and performance metrics/ETC calculation—into a single, **robust, highly configurable, and platform-agnostic core archival script**,
* **So that** it can be reliably executed for the complete HTML archival of The Magic Cafe forum, with the inherent flexibility to be run efficiently on my laptop for development, testing, or shorter archival runs, or be deployed for long-running, potentially unattended execution (e.g., on the Synology NAS, possibly within a Docker container).

**Acceptance Criteria (ACs):**

1.  **AC1:** A primary executable script (e.g., a main Go script, `run_archiver.go`) is created that orchestrates the end-to-end HTML archival process and is designed for platform-agnostic operation.
2.  **AC2:** The script successfully incorporates and utilizes the Topic Index consumption and prioritization logic (from Story 2.1).
3.  **AC3:** For each topic to be archived, the script correctly uses the intra-topic page navigation logic (from Story 2.2).
4.  **AC4:** For each identified topic page, the script uses the raw HTML page download functionality (from Story 2.3).
5.  **AC5:** The script correctly implements the HTML file storage and directory organization (using a configurable root path) as defined in Story 2.4.
6.  **AC6:** The script adheres to and utilizes the configurable "polite scraping" mechanisms (from Story 2.5).
7.  **AC7:** The script correctly implements and utilizes the resumable state management (from Story 2.6).
8.  **AC8:** The script accurately implements, tracks, and displays/logs the archival performance metrics and ETC calculations (from Story 2.7).
9.  **AC9:** Before starting the HTML archival process for the batch of topics belonging to a *specific sub-forum* (as per the master list from Epic 1), the script MUST perform a "Just-In-Time (JIT) Index Refresh." This involves re-scanning the first few (configurable number, e.g., 1-3) pages of that sub-forum's live topic listings to identify all visible Topic IDs. Any Topic IDs found during this JIT refresh that are *not already present in the master Topic ID list generated by Epic 1* are considered **newly created** and MUST be added to the immediate work queue for HTML archival along with the topics originally indexed for that sub-forum.
10. **AC10:** The core archival script MUST be highly configurable, primarily via command-line arguments, environment variables, or a configuration file, to allow for flexible operation on different platforms. Configurable parameters MUST include:
    * Path to the master Topic Index file(s).
    * Root output directory for the HTML archive (platform-agnostic path handling).
    * Key "polite scraping" parameters (e.g., request delay interval).
    * Path for the resumable state file.
    * Logging verbosity level and, optionally, a log file path.
    * Parameters for the JIT Index Refresh (e.g., number of initial pages of a sub-forum to re-scan).
11. **AC11:** The script is designed for robustness during potentially long-running tasks, including stable resource (CPU, memory) usage and comprehensive error handling for individual items that doesn't halt the entire batch if an error is localized.
12. **AC12:** The script produces comprehensive operational logs detailing its progress, key actions (like JIT refreshes), performance metrics, ETCs, and any errors encountered.
13. **AC13:** The script is designed to facilitate containerization (e.g., by reading configuration from environment variables or mountable config files, handling OS signals like SIGTERM for graceful shutdown).
14. **AC14:** Upon successful completion of an archival run (or a scheduled batch), the script has correctly archived the HTML for the processed topics and accurately updated its resumable state file.

---

**Story 2.9 (Revised AC1): Conduct Test Run & Refine Archival System**

* **As a** Developer/Operator of the Archival System,
* **I want** to execute the fully integrated Core Archival Script (developed in Story 2.8) on **the first few complete sub-forums from the prioritized list (ordered smallest to largest)** and meticulously analyze its end-to-end performance, the accuracy of its output, the functionality and tunability of its politeness mechanisms, its resumability, and the utility of its logging and metrics,
* **So that** I can comprehensively validate the entire archival system, gather initial real-world performance benchmarks for HTML archival based on processing complete sub-forums, and identify and implement any necessary refinements before initiating the full-scale forum archival (Story 2.10).

**Acceptance Criteria (ACs):**

1.  **AC1 (Revised):** The test batch for this run WILL consist of the **first 'N' complete sub-forums** from the prioritized list generated by Story 2.1 (which orders sub-forums from least topics to greatest). 'N' should be a small number (e.g., 3-5 sub-forums, or a number that constitutes a reasonable initial set of topics, perhaps 50-200 total topics across these initial sub-forums, to be decided by the operator) to allow for thorough testing of starting a sub-forum, completing all its topics, performing the JIT Index Refresh for each, and correctly transitioning to the next sub-forum.
2.  **AC2:** The Core Archival Script (from Story 2.8) is configured with the target test batch, appropriate initial politeness settings (e.g., a 3-5 second delay), and output locations for logs, state files, and archived HTML.
3.  **AC3:** The script is executed (e.g., on the Synology NAS or laptop, depending on where it's being developed/tested) and performs the complete HTML archival process for the test batch from start to finish.
4.  **AC4:** The script completes the test run for the batch without unhandled exceptions or critical errors that prematurely halt the entire process. Gracefully handled errors for individual items are logged as per plan.
5.  **AC5 (Politeness Tuning Test):** The configurability of the "polite scraping" delay (from Story 2.5) is explicitly tested. This involves:
    * Running the script on a small subset of the test batch with a conservative delay (e.g., 5 seconds) and noting the time taken and server responses (e.g., lack of errors).
    * Running the script on the same (or comparable) small subset with a more aggressive delay (e.g., 1-2 seconds) and carefully monitoring for any server errors (HTTP 429, 503, etc.) or increased page load times, to help determine an optimal but still polite setting.
6.  **AC6 (Resumability Test):** The resumability feature (from Story 2.6) is explicitly tested by:
    * Manually interrupting the script mid-way through processing the test batch.
    * Restarting the script and verifying that it correctly resumes from the point of interruption without re-downloading already archived pages and without skipping subsequent pages/topics in the batch.
7.  **AC7 (Output Verification):** A spot-check is performed on a sample of the archived HTML files from the test batch to ensure they are complete, unaltered, and correctly stored in the defined directory structure (as per Story 2.4).
8.  **AC8 (Metrics & ETC Validation):** The performance metrics (e.g., processing rate, total time taken) and the behavior of the Estimated Time to Completion (ETC) logged by the script during the test run (as per Story 2.7) are reviewed for accuracy and plausibility.
9.  **AC9 (Logging Review):** The operational logs generated during the test run are reviewed for clarity, informativeness, and to identify any unexpected warnings or behaviors.
10. **AC10 (Documentation of Findings):** Based on the outcomes of the test run (AC3-AC9), initial real-world performance benchmarks for HTML archival are documented. Any identified bugs, significant performance bottlenecks, inaccuracies, or deficiencies in any component (download, storage, politeness, resumability, metrics, JIT refresh) are documented.
11. **AC11 (Refinement Implementation):** Any critical or straightforward refinements identified in AC10 are implemented and re-tested within the scope of this story to ensure the Core Archival Script is robust and reliable for the full archival run. Major new features or complex changes identified are noted for potential future stories if not essential for the immediate full run.

---

**Story 2.10: Execute Full Forum Raw HTML Archival**

* **As an** Operator of the Archival System,
* **I want** to execute the fully tested, refined, and configured Core Archival Script (from Story 2.8, validated in Story 2.9) to systematically process all Topic IDs from the master index, downloading and saving the complete raw HTML for every page of every identified topic from The Magic Cafe forum,
* **So that** the "Waypoint Archive" on my Synology NAS contains a comprehensive and complete raw HTML backup of the entire forum, fulfilling the primary data preservation goal of this Epic.

**Acceptance Criteria (ACs):**

1.  **AC1:** The Core Archival Script (as validated and refined through Story 2.9) is configured with the master Topic Index (generated in Epic 1) and set to process all Topic IDs that have not yet been successfully archived (or to process the entire list if running from scratch with resumability).
2.  **AC2:** The script is launched on the chosen execution platform (e.g., Synology NAS for a long, unattended run, or the laptop, based on the decision made after Story 2.9's performance analysis) to begin the full archival process.
3.  **AC3:** The script systematically works through the prioritized list of Topic IDs (as prepared by Story 2.1), performing the "Just-In-Time (JIT) Index Refresh" for each new sub-forum's topics before processing them (as per Story 2.8 AC9).
4.  **AC4:** For each topic, the script correctly navigates all its internal pages (as per Story 2.2), downloads the raw HTML for each page (as per Story 2.3), and saves the HTML files to the designated location (e.g., Synology NAS) in the correct directory structure (as per Story 2.4).
5.  **AC5:** The script consistently applies the tuned "polite scraping" mechanisms (delay, User-Agent, as per Story 2.5) throughout the entire execution duration.
6.  **AC6:** The script correctly utilizes its resumable state management (as per Story 2.6), allowing for the process to be stopped and restarted (manually, by schedule, or due to unforeseen interruptions) and to continue from where it left off.
7.  **AC7:** Performance metrics and Estimated Time to Completion (ETC) (as per Story 2.7) continue to be logged throughout the full run, providing ongoing visibility into the progress of this potentially lengthy operation.
8.  **AC8:** The script runs until all Topic IDs in the master list have been processed, or until it is determined that all accessible content based on the provided index has been archived.
9.  **AC9:** Comprehensive operational logs are maintained for the entire full archival run, detailing overall progress, batches processed, any errors encountered (and how they were handled, e.g., skipped items after retries), and final completion statistics.
10. **AC10:** Upon completion of this story, the "Waypoint Archive" on the Synology NAS is confirmed to contain the raw HTML files for all successfully processed topics and their pages from the master index.
11. **AC11:** A final summary report or log entry indicates the total number of topics and pages successfully archived in this full run, the total duration, average performance metrics, and a list of any Topic IDs/pages that could not be archived after all defined error handling and retry attempts.

---